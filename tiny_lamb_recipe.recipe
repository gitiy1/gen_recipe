#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import re
from collections import OrderedDict
from urllib.parse import urljoin

from calibre.web.feeds.news import BasicNewsRecipe


class TinyLambRecipe(BasicNewsRecipe):
    # ========= 基本信息 =========
    title = u"基督教小小羊园地（按分类目录）"
    description = u"通过 RSS 抓取文章，再从文章页解析分类，按分类生成 EPUB 目录"
    language = "zh-CN"

    # ========= 站点配置 =========
    SITE_ROOT = "https://tiny-lamb-reformed.github.io"
    RSS_URL = "https://tiny-lamb-reformed.github.io/zh-cn/index.xml"

    # 每个分类最多抓多少篇（None/0 表示不限制）
    MAX_ARTICLES_PER_CATEGORY = 200
    # RSS 最多取多少篇（避免一次拉太多）
    MAX_TOTAL_ARTICLES = 800

    # ========= Calibre 行为 =========
    feeds = []  # 用 parse_index() 直接返回 (分类, 文章列表)
    remove_javascript = True
    no_stylesheets = False
    auto_cleanup = True
    ignore_duplicate_articles = True

    # ========= 工具函数 =========

    def _open_url(self, url):
        """
        mechanize 有时对目录型 URL 不走 index.html，做个兜底。
        """
        tried = []

        def try_one(u):
            tried.append(u)
            return self.browser.open(u).read()

        try:
            return try_one(url)
        except Exception:
            try:
                if url.endswith("/"):
                    return try_one(url + "index.html")
                return try_one(url + "/index.html")
            except Exception as e2:
                try:
                    self.log("Failed URLs: {}".format(tried))
                    self.log("Last error: {}".format(e2))
                except Exception:
                    pass
                raise

    def _soup(self, url_or_bytes):
        from bs4 import BeautifulSoup
        if isinstance(url_or_bytes, (bytes, bytearray)):
            return BeautifulSoup(url_or_bytes, "html.parser")
        raw = self._open_url(url_or_bytes)
        return BeautifulSoup(raw, "html.parser")

    def _norm_ws(self, s):
        return re.sub(r"\s+", " ", (s or "").strip())

    def _pick_categories_from_article_html(self, article_url):
        """
        从文章页解析分类：
        - 常见形式：页面里会有指向 /zh-cn/categories/<something>/ 的 <a>
        - 我们抓这些链接的文本作为分类名
        """
        soup = self._soup(article_url)
        cats = []

        for a in soup.find_all("a", href=True):
            href = (a.get("href") or "").strip()
            if "/zh-cn/categories/" not in href:
                continue
            # 过滤掉 categories 索引页本身
            if href.rstrip("/").endswith("/zh-cn/categories"):
                continue
            txt = self._norm_ws(a.get_text())
            if not txt:
                continue
            cats.append(txt)

        # 去重保持顺序
        out = []
        seen = set()
        for c in cats:
            if c not in seen:
                seen.add(c)
                out.append(c)

        return out

    def _parse_rss_entries(self, rss_bytes):
        """
        尽量用 feedparser；如果环境里没有就 fallback 用简单正则解析。
        返回 [(title, link, date_str), ...]
        """
        # 1) feedparser
        try:
            import feedparser  # calibre 环境通常有
            f = feedparser.parse(rss_bytes)
            entries = []
            for e in f.entries:
                title = self._norm_ws(getattr(e, "title", ""))
                link = getattr(e, "link", "") or ""
                # published / updated 取一个
                date = getattr(e, "published", "") or getattr(e, "updated", "") or ""
                date = self._norm_ws(date)
                if title and link:
                    entries.append((title, link, date))
            return entries
        except Exception:
            pass

        # 2) fallback: 简单从 <item> 里抓 <title><link><pubDate>（够用）
        txt = rss_bytes.decode("utf-8", "ignore")
        items = re.findall(r"<item\b.*?>.*?</item>", txt, flags=re.S | re.I)
        entries = []
        for it in items:
            t = re.search(r"<title\b.*?>(.*?)</title>", it, flags=re.S | re.I)
            l = re.search(r"<link\b.*?>(.*?)</link>", it, flags=re.S | re.I)
            d = re.search(r"<pubDate\b.*?>(.*?)</pubDate>", it, flags=re.S | re.I)
            title = self._norm_ws(t.group(1)) if t else ""
            link = self._norm_ws(l.group(1)) if l else ""
            date = self._norm_ws(d.group(1)) if d else ""
            if title and link:
                entries.append((title, link, date))
        return entries

    # ========= 核心逻辑 =========

    def parse_index(self):
        """
        通过 RSS 得到文章列表 -> 逐篇解析分类 -> 按分类分组返回
        返回格式：
        [
          (分类名, [ {title,url,date,description}, ... ]),
          ...
        ]
        """
        rss_bytes = self._open_url(self.RSS_URL)
        entries = self._parse_rss_entries(rss_bytes)

        if self.MAX_TOTAL_ARTICLES and len(entries) > self.MAX_TOTAL_ARTICLES:
            entries = entries[: self.MAX_TOTAL_ARTICLES]

        # 分类 -> articles（OrderedDict 保持插入顺序）
        grouped = OrderedDict()

        for (title, link, date) in entries:
            # 统一成绝对 URL
            url = urljoin(self.SITE_ROOT, link)

            # 从文章页解析分类
            try:
                cats = self._pick_categories_from_article_html(url)
            except Exception:
                cats = []

            if not cats:
                cats = [u"未分类"]

            # 多分类文章：为了目录完整性，这里“复制到每个分类”
            # 如果你不想重复，可改成只取 cats[0]
            for cat in cats:
                grouped.setdefault(cat, [])
                grouped[cat].append({
                    "title": title,
                    "url": url,
                    "date": date,
                    "description": "",
                })

        # 每个分类限制篇数
        feeds = []
        for cat, arts in grouped.items():
            if self.MAX_ARTICLES_PER_CATEGORY:
                arts = arts[: self.MAX_ARTICLES_PER_CATEGORY]
            if arts:
                feeds.append((cat, arts))

        return feeds
