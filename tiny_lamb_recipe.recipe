#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import re
from collections import OrderedDict
from urllib.parse import urljoin, urlparse

from calibre.web.feeds.news import BasicNewsRecipe


class TinyLambRecipe(BasicNewsRecipe):
    title = u"基督教小小羊园地（按分类目录）"
    description = u"RSS 拉文章 -> 文章页解析分类 -> 按分类生成 EPUB 目录（统一 zh-cn 链接并去重）"
    language = "zh-CN"

    SITE_ROOT = "https://tiny-lamb-reformed.github.io"
    RSS_URL = "https://tiny-lamb-reformed.github.io/zh-cn/index.xml"

    MAX_TOTAL_ARTICLES = 800
    MAX_ARTICLES_PER_CATEGORY = 200

    # 不要设置 ignore_duplicate_articles（你这版 calibre 里不是 bool）
    # ignore_duplicate_articles = True  # <-- 删掉/不要写

    remove_javascript = True
    no_stylesheets = False
    auto_cleanup = True

    feeds = []  # 用 parse_index() 直接返回 (分类, 文章列表)

    # ---------- 网络/解析 ----------

    def _open_url(self, url):
        # mechanize 有时对目录 URL 不自动 index.html，做兜底
        tried = []

        def try_one(u):
            tried.append(u)
            return self.browser.open(u).read()

        try:
            return try_one(url)
        except Exception:
            try:
                if url.endswith("/"):
                    return try_one(url + "index.html")
                return try_one(url + "/index.html")
            except Exception as e2:
                try:
                    self.log("Failed URLs: {}".format(tried))
                    self.log("Last error: {}".format(e2))
                except Exception:
                    pass
                raise

    def _soup(self, url):
        raw = self._open_url(url)
        from bs4 import BeautifulSoup
        return BeautifulSoup(raw, "html.parser")

    def _norm_ws(self, s):
        return re.sub(r"\s+", " ", (s or "").strip())

    # ---------- 关键：URL 规范化，统一到 /zh-cn/posts/ ----------

    def _canonical_post_url(self, link):
        """
        把各种形式统一为：
        https://tiny-lamb-reformed.github.io/zh-cn/posts/<slug>/
        用来避免 /posts/ 与 /zh-cn/posts/ 的重复下载与标题异常。
        """
        if not link:
            return None

        # 先变成绝对 URL
        url = urljoin(self.SITE_ROOT, link)
        p = urlparse(url)
        path = p.path or ""

        # 识别 posts slug（支持 /posts/x/ 和 /zh-cn/posts/x/）
        m = re.search(r"/(?:zh-cn/)?posts/([^/]+)/?", path)
        if not m:
            return url  # 非 posts 链接就原样返回

        slug = m.group(1)
        return "{}/zh-cn/posts/{}/".format(self.SITE_ROOT, slug)

    # ---------- RSS 解析 ----------

    def _parse_rss_entries(self, rss_bytes):
        """
        返回 [(title, link, date_str), ...]
        优先 feedparser，fallback 简单正则。
        """
        try:
            import feedparser
            f = feedparser.parse(rss_bytes)
            out = []
            for e in f.entries:
                title = self._norm_ws(getattr(e, "title", ""))
                link = getattr(e, "link", "") or ""
                date = getattr(e, "published", "") or getattr(e, "updated", "") or ""
                date = self._norm_ws(date)
                if title and link:
                    out.append((title, link, date))
            return out
        except Exception:
            pass

        txt = rss_bytes.decode("utf-8", "ignore")
        items = re.findall(r"<item\b.*?>.*?</item>", txt, flags=re.S | re.I)
        out = []
        for it in items:
            t = re.search(r"<title\b.*?>(.*?)</title>", it, flags=re.S | re.I)
            l = re.search(r"<link\b.*?>(.*?)</link>", it, flags=re.S | re.I)
            d = re.search(r"<pubDate\b.*?>(.*?)</pubDate>", it, flags=re.S | re.I)
            title = self._norm_ws(t.group(1)) if t else ""
            link = self._norm_ws(l.group(1)) if l else ""
            date = self._norm_ws(d.group(1)) if d else ""
            if title and link:
                out.append((title, link, date))
        return out

    # ---------- 文章页分类解析 ----------

    def _pick_categories_from_article(self, article_url):
        """
        从文章页解析分类：抓所有 href 含 /zh-cn/categories/ 的 a 标签文本
        """
        soup = self._soup(article_url)
        cats = []

        for a in soup.find_all("a", href=True):
            href = (a.get("href") or "").strip()
            if "/zh-cn/categories/" not in href:
                continue
            # 排除 categories 首页
            if href.rstrip("/").endswith("/zh-cn/categories"):
                continue
            txt = self._norm_ws(a.get_text())
            if txt:
                cats.append(txt)

        # 去重保持顺序
        out = []
        seen = set()
        for c in cats:
            if c not in seen:
                seen.add(c)
                out.append(c)

        return out

    # ---------- 核心：构建分类目录 ----------

    def parse_index(self):
        rss_bytes = self._open_url(self.RSS_URL)
        entries = self._parse_rss_entries(rss_bytes)

        if self.MAX_TOTAL_ARTICLES and len(entries) > self.MAX_TOTAL_ARTICLES:
            entries = entries[: self.MAX_TOTAL_ARTICLES]

        grouped = OrderedDict()
        global_seen = set()  # 用规范化 URL 做全局去重（避免同文两次）

        for (title, link, date) in entries:
            canon_url = self._canonical_post_url(link)
            if not canon_url:
                continue

            # 全局去重
            if canon_url in global_seen:
                continue
            global_seen.add(canon_url)

            # 解析分类
            try:
                cats = self._pick_categories_from_article(canon_url)
            except Exception:
                cats = []

            if not cats:
                cats = [u"未分类"]

            # 多分类文章：默认放到所有分类目录（可改成只取第一个）
            for cat in cats:
                grouped.setdefault(cat, [])
                grouped[cat].append({
                    "title": title,          # 用 RSS 标题（你说这一套是正常的）
                    "url": canon_url,        # 关键：统一 zh-cn
                    "date": date,
                    "description": "",
                })

        feeds = []
        for cat, arts in grouped.items():
            if self.MAX_ARTICLES_PER_CATEGORY:
                arts = arts[: self.MAX_ARTICLES_PER_CATEGORY]
            if arts:
                feeds.append((cat, arts))

        return feeds
