#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import re
from urllib.parse import urljoin

from calibre.web.feeds.news import BasicNewsRecipe


class TinyLambRecipe(BasicNewsRecipe):

    # ========= 基本信息 =========
    title = u"基督教小小羊园地（按分类目录）"
    description = u"从分类页面抓取文章，按分类生成 EPUB 目录"
    language = "zh-CN"

    # ========= 站点配置 =========
    SITE_ROOT = "https://tiny-lamb-reformed.github.io"
    CATEGORIES_INDEX = "https://tiny-lamb-reformed.github.io/zh-cn/categories/"

    # 每个分类最多抓多少篇（None 或 0 表示不限制）
    MAX_ARTICLES_PER_CATEGORY = 200

    # 每个分类最多翻多少页（防止死循环）
    MAX_PAGES_PER_CATEGORY = 50

    # ========= Calibre 行为 =========
    feeds = []                  # 不用 RSS feeds，改用 parse_index
    remove_javascript = True
    no_stylesheets = False
    auto_cleanup = True

    # ========= 工具函数 =========

    def _soup(self, url):
        """
        拉取页面并返回 BeautifulSoup
        使用 html.parser，避免 runner 没装 lxml
        """
        raw = self.browser.open(url).read()
        from bs4 import BeautifulSoup
        return BeautifulSoup(raw, "html.parser")

    def _clean_category_title(self, text):
        """
        分类名形如：'教会流行观念的偏差 493'
        去掉末尾数字
        """
        if not text:
            return None
        t = re.sub(r"\s+", " ", text).strip()
        t = re.sub(r"\s+\d+\s*$", "", t)
        return t.strip() or None

    def _find_next_page(self, soup, current_url):
        """
        查找“下一页 →”
        """
        for a in soup.find_all("a", href=True):
            txt = a.get_text(" ", strip=True) or ""
            if "下一页" in txt:
                return urljoin(current_url, a["href"])
        return None

    def _extract_date_near_link(self, a):
        """
        从文章链接附近文本中找 YYYY-MM-DD
        """
        try:
            texts = []
            if a.parent:
                texts.append(a.parent.get_text(" ", strip=True))
            if a.parent and a.parent.parent:
                texts.append(a.parent.parent.get_text(" ", strip=True))
            block = " ".join(texts)
        except Exception:
            block = ""
        m = re.search(r"\b(\d{4}-\d{2}-\d{2})\b", block)
        return m.group(1) if m else ""

    def _extract_description_near_link(self, a):
        """
        尝试拿分类页中的摘要
        """
        try:
            parent = a.parent
            text = parent.get_text(" ", strip=True) if parent else ""
            title = a.get_text(" ", strip=True)
            if title and title in text:
                text = text.replace(title, "", 1)
            text = re.sub(r"\s+", " ", text).strip(" -|•\u00a0")
            return text[:240]
        except Exception:
            return ""

    # ========= 核心逻辑 =========

    def parse_index(self):
        """
        返回：
        [
          (分类名, [
              {title, url, date, description},
              ...
          ]),
          ...
        ]
        —— 每个分类就是 EPUB 的一级目录
        """

        index_soup = self._soup(self.CATEGORIES_INDEX)

        # ---- 1. 抓分类列表 ----
        categories = []
        seen_cat_urls = set()

        for a in index_soup.find_all("a", href=True):
            href = (a.get("href") or "").strip()
            if "/zh-cn/categories/" not in href:
                continue

            cat_url = urljoin(self.CATEGORIES_INDEX, href).split("#", 1)[0]
            if cat_url.rstrip("/") == self.CATEGORIES_INDEX.rstrip("/"):
                continue
            if not cat_url.startswith(self.CATEGORIES_INDEX):
                continue

            cat_title = self._clean_category_title(
                a.get_text(" ", strip=True)
            )
            if not cat_title:
                continue

            if cat_url in seen_cat_urls:
                continue

            seen_cat_urls.add(cat_url)
            categories.append((cat_title, cat_url))

        # ---- 2. 逐分类抓文章 ----
        feeds = []
        post_href_re = re.compile(r"/posts/")

        for cat_title, cat_url in categories:
            articles = []
            seen_posts = set()

            page_url = cat_url
            pages = 0

            while page_url and pages < self.MAX_PAGES_PER_CATEGORY:
                pages += 1
                soup = self._soup(page_url)

                for a in soup.find_all("a", href=True):
                    href = (a.get("href") or "").strip()
                    if not post_href_re.search(href):
                        continue

                    post_url = urljoin(page_url, href).split("#", 1)[0]
                    if not post_url.startswith(self.SITE_ROOT):
                        continue
                    if post_url in seen_posts:
                        continue

                    title = re.sub(
                        r"\s+",
                        " ",
                        a.get_text(" ", strip=True)
                    ).strip()
                    if not title:
                        continue

                    seen_posts.add(post_url)
                    articles.append({
                        "title": title,
                        "url": post_url,
                        "date": self._extract_date_near_link(a),
                        "description": self._extract_description_near_link(a),
                    })

                    if (
                        self.MAX_ARTICLES_PER_CATEGORY
                        and len(articles) >= self.MAX_ARTICLES_PER_CATEGORY
                    ):
                        break

                if (
                    self.MAX_ARTICLES_PER_CATEGORY
                    and len(articles) >= self.MAX_ARTICLES_PER_CATEGORY
                ):
                    break

                next_url = self._find_next_page(soup, page_url)
                if next_url and next_url != page_url:
                    page_url = next_url
                else:
                    page_url = None

            if articles:
                feeds.append((cat_title, articles))

        return feeds
