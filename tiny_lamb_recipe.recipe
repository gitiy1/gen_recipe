#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import re
from collections import OrderedDict
from urllib.parse import urljoin, urlparse

from calibre.web.feeds.news import BasicNewsRecipe


class TinyLambRecipe(BasicNewsRecipe):
    title = u"基督教小小羊园地（按分类目录）"
    description = u"RSS 拉文章 -> 文章页解析分类 -> 按分类生成 EPUB 目录（仅保留 post-heading 与 blog-post）"
    language = "zh-CN"

    SITE_ROOT = "https://tiny-lamb-reformed.github.io"
    RSS_URL = "https://tiny-lamb-reformed.github.io/zh-cn/index.xml"

    MAX_TOTAL_ARTICLES = 800
    MAX_ARTICLES_PER_CATEGORY = 200

    no_stylesheets = True
    remove_javascript = True
    compress_news_images = True
    scale_news_images = (800, 1000)
    remove_attributes = ['style', 'width', 'height', 'align']
    auto_cleanup = False  # 我们自己精确裁剪 HTML

    feeds = []  # 不用内置 feeds

    # -----------------------
    # 网络 / 解析
    # -----------------------

    def _open_url(self, url):
        # mechanize 对目录 URL 有时 404，做 index.html 兜底
        tried = []

        def try_one(u):
            tried.append(u)
            return self.browser.open(u).read()

        try:
            return try_one(url)
        except Exception:
            try:
                if url.endswith("/"):
                    return try_one(url + "index.html")
                return try_one(url + "/index.html")
            except Exception as e2:
                try:
                    self.log("Failed URLs: {}".format(tried))
                    self.log("Last error: {}".format(e2))
                except Exception:
                    pass
                raise

    def _soup_from_bytes(self, raw):
        from bs4 import BeautifulSoup
        return BeautifulSoup(raw, "html.parser")

    def _soup(self, url):
        return self._soup_from_bytes(self._open_url(url))

    def _norm_ws(self, s):
        return re.sub(r"\s+", " ", (s or "").strip())

    # -----------------------
    # 关键：只保留两个块
    # -----------------------

    def preprocess_html(self, soup):
        """
        对每篇文章生效：只保留 class=post-heading 与 class=blog-post
        """
        try:
            body = soup.body
            if body is None:
                return soup

            keep = []
            for cls in ("post-heading", "blog-post"):
                keep.extend(soup.find_all(class_=cls))

            # 找不到就不裁剪，避免输出空白
            if not keep:
                return soup

            body.clear()
            for tag in keep:
                body.append(tag)

        except Exception:
            return soup

        return soup

    # -----------------------
    # URL 统一 / 去重
    # -----------------------

    def _canonical_post_url(self, link):
        """
        把 /posts/<slug>/ 和 /zh-cn/posts/<slug>/ 全部统一为 /zh-cn/posts/<slug>/
        解决你说的：同文下载两次 + /posts/ 那套标题繁中/异常
        """
        if not link:
            return None

        url = urljoin(self.SITE_ROOT, link)
        p = urlparse(url)
        path = p.path or ""

        m = re.search(r"/(?:zh-cn/)?posts/([^/]+)/?", path)
        if not m:
            return url

        slug = m.group(1)
        return "{}/zh-cn/posts/{}/".format(self.SITE_ROOT, slug)

    # -----------------------
    # RSS 解析
    # -----------------------

    def _parse_rss_entries(self, rss_bytes):
        """
        返回 [(title, link, date_str), ...]
        """
        try:
            import feedparser
            f = feedparser.parse(rss_bytes)
            out = []
            for e in f.entries:
                title = self._norm_ws(getattr(e, "title", ""))
                link = getattr(e, "link", "") or ""
                date = getattr(e, "published", "") or getattr(e, "updated", "") or ""
                date = self._norm_ws(date)
                if title and link:
                    out.append((title, link, date))
            return out
        except Exception:
            pass

        # fallback：简单正则
        txt = rss_bytes.decode("utf-8", "ignore")
        items = re.findall(r"<item\b.*?>.*?</item>", txt, flags=re.S | re.I)
        out = []
        for it in items:
            t = re.search(r"<title\b.*?>(.*?)</title>", it, flags=re.S | re.I)
            l = re.search(r"<link\b.*?>(.*?)</link>", it, flags=re.S | re.I)
            d = re.search(r"<pubDate\b.*?>(.*?)</pubDate>", it, flags=re.S | re.I)
            title = self._norm_ws(t.group(1)) if t else ""
            link = self._norm_ws(l.group(1)) if l else ""
            date = self._norm_ws(d.group(1)) if d else ""
            if title and link:
                out.append((title, link, date))
        return out

    # -----------------------
    # 从文章页解析分类（用于目录分组）
    # -----------------------

    def _pick_categories_from_article(self, article_url):
        soup = self._soup(article_url)

        cats = []
        for a in soup.find_all("a", href=True):
            href = (a.get("href") or "").strip()
            if "/zh-cn/categories/" not in href:
                continue
            if href.rstrip("/").endswith("/zh-cn/categories"):
                continue
            txt = self._norm_ws(a.get_text())
            if txt:
                cats.append(txt)

        # 去重保持顺序
        out, seen = [], set()
        for c in cats:
            if c not in seen:
                seen.add(c)
                out.append(c)
        return out

    # -----------------------
    # 目录分组：分类 -> 文章列表
    # -----------------------

    def parse_index(self):
        rss_bytes = self._open_url(self.RSS_URL)
        entries = self._parse_rss_entries(rss_bytes)

        if self.MAX_TOTAL_ARTICLES and len(entries) > self.MAX_TOTAL_ARTICLES:
            entries = entries[: self.MAX_TOTAL_ARTICLES]

        grouped = OrderedDict()
        global_seen = set()  # 规范化 URL 全局去重

        for (title, link, date) in entries:
            canon_url = self._canonical_post_url(link)
            if not canon_url:
                continue

            if canon_url in global_seen:
                continue
            global_seen.add(canon_url)

            try:
                cats = self._pick_categories_from_article(canon_url)
            except Exception:
                cats = []

            if not cats:
                cats = [u"未分类"]

            # 多分类文章：放到所有分类目录
            for cat in cats:
                grouped.setdefault(cat, [])
                grouped[cat].append({
                    "title": title,      # 用 RSS 标题（你说这套是正常的）
                    "url": canon_url,    # 统一 zh-cn，避免 /posts/ 版本
                    "date": date,
                    "description": "",
                })

        feeds = []
        for cat, arts in grouped.items():
            if self.MAX_ARTICLES_PER_CATEGORY:
                arts = arts[: self.MAX_ARTICLES_PER_CATEGORY]
            if arts:
                feeds.append((cat, arts))
        return feeds

    # -----------------------
    # 关键修复：你这版 calibre 会走 parse_feeds()->get_feeds()
    # 所以我们重写 parse_feeds()，让它用 parse_index() 构建 Feed 对象
    # -----------------------

    def parse_feeds(self):
        """
        把 parse_index() 的 (title, [article dict...]) 转成 calibre Feed 对象列表。
        这样 build_index() 即使走 parse_feeds，也不会触发 get_feeds()。
        """
        from calibre.web.feeds.news import Feed

        out = []
        for feed_title, articles in self.parse_index():
            f = Feed(feed_title, articles)
            out.append(f)
        return out
