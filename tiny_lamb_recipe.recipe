#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import re
from urllib.parse import urljoin, quote

from calibre.web.feeds.news import BasicNewsRecipe


class TinyLambRecipe(BasicNewsRecipe):
    # ========= 基本信息 =========
    title = u"基督教小小羊园地（按分类目录）"
    description = u"从分类页面抓取分类结构，按分类生成 EPUB 目录"
    language = "zh-CN"

    # ========= 站点配置 =========
    SITE_ROOT = "https://tiny-lamb-reformed.github.io"
    CATEGORIES_INDEX = "https://tiny-lamb-reformed.github.io/zh-cn/categories/"

    # 每个分类最多抓多少篇（None/0 表示不限制）
    MAX_ARTICLES_PER_CATEGORY = 0
    # 每个分类最多翻多少页（防止死循环）
    MAX_PAGES_PER_CATEGORY = 50

    # ========= Calibre 行为 =========
    feeds = []  # 不走 RSS feeds，走 parse_index
    remove_javascript = True
    no_stylesheets = False
    auto_cleanup = True

    # ========= 工具函数 =========

    def _open_url(self, url):
        """
        mechanize 在 GitHub Pages 上对“目录型 URL”可能 404。
        这里做容错：
        1) 试 url
        2) 试 url + 'index.html'
        """
        tried = []

        def try_one(u):
            tried.append(u)
            return self.browser.open(u).read()

        # 1) 原 URL
        try:
            return try_one(url)
        except Exception as e1:
            # 2) index.html 兜底
            try:
                if url.endswith("/"):
                    return try_one(url + "index.html")
                else:
                    return try_one(url + "/index.html")
            except Exception as e2:
                # 打印更有用的日志
                try:
                    self.log("Failed URLs: {}".format(tried))
                    self.log("Last error: {}".format(e2))
                except Exception:
                    pass
                raise

    def _soup(self, url):
        raw = self._open_url(url)
        from bs4 import BeautifulSoup
        return BeautifulSoup(raw, "html.parser")

    def _clean_category_title(self, text):
        """
        分类名形如：'教会流行观念的偏差 493'
        去掉末尾计数，返回 '教会流行观念的偏差'
        """
        if not text:
            return None
        t = re.sub(r"\s+", " ", text).strip()
        t = re.sub(r"\s+\d+\s*$", "", t).strip()
        return t or None

    def _category_term_url_candidates(self, cat_title):
        """
        Hugo taxonomy term URL：/zh-cn/categories/<term>/
        但 GitHub Pages + mechanize 有时对编码/目录映射不一致：
        - 试 percent-encoding
        - 试原始中文路径
        - 两者都再试 index.html（由 _open_url 负责）
        """
        encoded = quote(cat_title, safe="")
        u1 = urljoin(self.CATEGORIES_INDEX, encoded + "/")
        u2 = urljoin(self.CATEGORIES_INDEX, cat_title + "/")
        # 去重保持顺序
        out = []
        for u in (u1, u2):
            if u not in out:
                out.append(u)
        return out

    def _find_next_page(self, soup, current_url):
        """
        在分类页底部找 “下一页 →”
        """
        for a in soup.find_all("a", href=True):
            txt = a.get_text(" ", strip=True) or ""
            if "下一页" in txt:
                return urljoin(current_url, a["href"])
        return None

    def _extract_date_near_link(self, a):
        """
        从文章链接附近文本中找 YYYY-MM-DD
        """
        try:
            texts = []
            if a.parent:
                texts.append(a.parent.get_text(" ", strip=True))
            if a.parent and a.parent.parent:
                texts.append(a.parent.parent.get_text(" ", strip=True))
            block = " ".join(texts)
        except Exception:
            block = ""
        m = re.search(r"\b(\d{4}-\d{2}-\d{2})\b", block)
        return m.group(1) if m else ""

    def _extract_description_near_link(self, a):
        """
        尝试拿分类页中的摘要（抓不到也没关系）
        """
        try:
            parent = a.parent
            text = parent.get_text(" ", strip=True) if parent else ""
            title = a.get_text(" ", strip=True)
            if title and title in text:
                text = text.replace(title, "", 1)
            text = re.sub(r"\s+", " ", text).strip(" -|•\u00a0")
            return text[:240]
        except Exception:
            return ""

    def _discover_categories(self, index_soup):
        """
        /zh-cn/categories/ 页面分类列表是纯文本 li（不是链接）。
        从 li 文本解析分类名，然后自己拼 term URL。
        """
        categories = []
        seen_titles = set()

        for li in index_soup.find_all("li"):
            txt = li.get_text(" ", strip=True) or ""
            # 典型分类行末尾是数字计数
            if not re.search(r"\s\d+\s*$", txt):
                continue
            cat_title = self._clean_category_title(txt)
            if not cat_title:
                continue
            if cat_title in seen_titles:
                continue
            seen_titles.add(cat_title)
            categories.append(cat_title)

        return categories

    # ========= 核心逻辑 =========

    def parse_index(self):
        """
        返回：
        [
          (分类名, [ {title,url,date,description}, ... ]),
          ...
        ]
        —— 每个分类就是 EPUB 的一级目录
        """
        # 先抓分类索引（带 index.html 容错）
        index_soup = self._soup(self.CATEGORIES_INDEX)
        cat_titles = self._discover_categories(index_soup)

        feeds = []
        post_href_re = re.compile(r"/posts/")

        for cat_title in cat_titles:
            # 找到一个能打开的分类 URL（编码/不编码 两种都试）
            cat_url = None
            for cand in self._category_term_url_candidates(cat_title):
                try:
                    # 试探性打开，能打开就用
                    self._open_url(cand)
                    cat_url = cand
                    break
                except Exception:
                    continue

            if not cat_url:
                # 这个分类打不开就跳过
                try:
                    self.log("Skip category (cannot open): {}".format(cat_title))
                except Exception:
                    pass
                continue

            articles = []
            seen_posts = set()

            page_url = cat_url
            pages = 0

            while page_url and pages < self.MAX_PAGES_PER_CATEGORY:
                pages += 1
                soup = self._soup(page_url)

                for a in soup.find_all("a", href=True):
                    href = (a.get("href") or "").strip()
                    # 文章链接：匹配 /posts/（可覆盖 ../../posts/...）
                    if not post_href_re.search(href):
                        continue

                    post_url = urljoin(page_url, href).split("#", 1)[0]
                    if not post_url.startswith(self.SITE_ROOT):
                        continue
                    if post_url in seen_posts:
                        continue

                    title = re.sub(r"\s+", " ", a.get_text(" ", strip=True)).strip()
                    if not title:
                        continue

                    seen_posts.add(post_url)
                    articles.append({
                        "title": title,
                        "url": post_url,
                        "date": self._extract_date_near_link(a),
                        "description": self._extract_description_near_link(a),
                    })

                    if self.MAX_ARTICLES_PER_CATEGORY and len(articles) >= self.MAX_ARTICLES_PER_CATEGORY:
                        break

                if self.MAX_ARTICLES_PER_CATEGORY and len(articles) >= self.MAX_ARTICLES_PER_CATEGORY:
                    break

                next_url = self._find_next_page(soup, page_url)
                if next_url and next_url != page_url:
                    page_url = next_url
                else:
                    page_url = None

            if articles:
                feeds.append((cat_title, articles))
            else:
                try:
                    self.log("Category has no articles (parsed): {}".format(cat_title))
                except Exception:
                    pass

        return feeds
