#!/usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import unicode_literals

import re
from collections import OrderedDict
from urllib.parse import urljoin, urlparse

from calibre.web.feeds.news import BasicNewsRecipe


class TinyLambRecipe(BasicNewsRecipe):
    title = u"基督教小小羊园地（按分类目录）"
    description = u"RSS 拉文章 -> 文章页解析分类 -> 按分类生成 EPUB 目录（正文仅保留 post-heading 与 blog-post）"
    language = "zh-CN"

    SITE_ROOT = "https://tiny-lamb-reformed.github.io"
    RSS_URL = "https://tiny-lamb-reformed.github.io/zh-cn/index.xml"

    MAX_TOTAL_ARTICLES = 800
    MAX_ARTICLES_PER_CATEGORY = 200

    # 注意：这里必须是集合/可迭代，不要写 True
    ignore_duplicate_articles = {'url'}  # 文档示例就是这样写的 :contentReference[oaicite:2]{index=2}

    remove_javascript = True
    no_stylesheets = False
    auto_cleanup = False  # 我们要精确保留两个块，不用 readability 自动清理

    feeds = []

    # ---------- 网络/解析 ----------

    def _open_url(self, url):
        tried = []

        def try_one(u):
            tried.append(u)
            return self.browser.open(u).read()

        try:
            return try_one(url)
        except Exception:
            try:
                if url.endswith("/"):
                    return try_one(url + "index.html")
                return try_one(url + "/index.html")
            except Exception as e2:
                try:
                    self.log("Failed URLs: {}".format(tried))
                    self.log("Last error: {}".format(e2))
                except Exception:
                    pass
                raise

    def _soup(self, url):
        raw = self._open_url(url)
        from bs4 import BeautifulSoup
        return BeautifulSoup(raw, "html.parser")

    def _norm_ws(self, s):
        return re.sub(r"\s+", " ", (s or "").strip())

    # ---------- 关键：正文只保留两块 ----------

    def preprocess_html(self, soup):
        """
        只保留 class=post-heading 和 class=blog-post 的内容。
        这是对每篇文章页生效的强制清洗。:contentReference[oaicite:3]{index=3}
        """
        try:
            body = soup.body
            if body is None:
                return soup

            keep = []

            # 先标题块，再正文块（保持阅读顺序）
            for cls in ("post-heading", "post-meta", "blog-post"):
                for tag in soup.find_all(class_=cls):
                    keep.append(tag)

            # 如果完全没找到（比如页面结构变了），就别清空，避免输出空白文章
            if not keep:
                return soup

            body.clear()
            for tag in keep:
                body.append(tag)

        except Exception:
            # 出错时宁可保留原文，避免整篇文章空掉
            return soup

        return soup

    # ---------- URL 规范化，统一到 /zh-cn/posts/ ----------

    def _canonical_post_url(self, link):
        if not link:
            return None
        url = urljoin(self.SITE_ROOT, link)
        p = urlparse(url)
        path = p.path or ""

        m = re.search(r"/(?:zh-cn/)?posts/([^/]+)/?", path)
        if not m:
            return url

        slug = m.group(1)
        return "{}/zh-cn/posts/{}/".format(self.SITE_ROOT, slug)

    # ---------- RSS 解析 ----------

    def _parse_rss_entries(self, rss_bytes):
        try:
            import feedparser
            f = feedparser.parse(rss_bytes)
            out = []
            for e in f.entries:
                title = self._norm_ws(getattr(e, "title", ""))
                link = getattr(e, "link", "") or ""
                date = getattr(e, "published", "") or getattr(e, "updated", "") or ""
                date = self._norm_ws(date)
                if title and link:
                    out.append((title, link, date))
            return out
        except Exception:
            pass

        txt = rss_bytes.decode("utf-8", "ignore")
        items = re.findall(r"<item\b.*?>.*?</item>", txt, flags=re.S | re.I)
        out = []
        for it in items:
            t = re.search(r"<title\b.*?>(.*?)</title>", it, flags=re.S | re.I)
            l = re.search(r"<link\b.*?>(.*?)</link>", it, flags=re.S | re.I)
            d = re.search(r"<pubDate\b.*?>(.*?)</pubDate>", it, flags=re.S | re.I)
            title = self._norm_ws(t.group(1)) if t else ""
            link = self._norm_ws(l.group(1)) if l else ""
            date = self._norm_ws(d.group(1)) if d else ""
            if title and link:
                out.append((title, link, date))
